{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "fastaitest.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bt4O_-wqJGCP",
        "outputId": "a4a2cc1a-9cce-4e27-b21d-0d881403de2f"
      },
      "source": [
        "!pip install fastai --upgrade"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fastai\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e8/79/e8a87e4c20238e114671314426227db8647d2b42744eab79e0917c59865e/fastai-2.3.1-py3-none-any.whl (194kB)\n",
            "\r\u001b[K     |█▊                              | 10kB 15.7MB/s eta 0:00:01\r\u001b[K     |███▍                            | 20kB 20.0MB/s eta 0:00:01\r\u001b[K     |█████                           | 30kB 23.8MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 40kB 26.9MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 51kB 28.4MB/s eta 0:00:01\r\u001b[K     |██████████                      | 61kB 30.4MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 71kB 31.3MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 81kB 32.7MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 92kB 32.1MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 102kB 33.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 112kB 33.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 122kB 33.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 133kB 33.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 143kB 33.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 153kB 33.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 163kB 33.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 174kB 33.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 184kB 33.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 194kB 33.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 204kB 33.3MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.7/dist-packages (from fastai) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: pandas in /usr/local/lib/python3.7/dist-packages (from fastai) (1.1.5)\n",
            "Requirement already satisfied, skipping upgrade: pillow>6.0.0 in /usr/local/lib/python3.7/dist-packages (from fastai) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.7/dist-packages (from fastai) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: torchvision>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from fastai) (0.9.1+cu101)\n",
            "Requirement already satisfied, skipping upgrade: torch<1.9,>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from fastai) (1.8.1+cu101)\n",
            "Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.7/dist-packages (from fastai) (20.9)\n",
            "Collecting fastcore<1.4,>=1.3.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b0/f1fbf554e0bf3c76e1bdc3b82eedfe41fcf656479586be38c64421082b1b/fastcore-1.3.20-py3-none-any.whl (53kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 9.4MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: fastprogress>=0.2.4 in /usr/local/lib/python3.7/dist-packages (from fastai) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.7/dist-packages (from fastai) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: matplotlib in /usr/local/lib/python3.7/dist-packages (from fastai) (3.2.2)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn in /usr/local/lib/python3.7/dist-packages (from fastai) (0.22.2.post1)\n",
            "Requirement already satisfied, skipping upgrade: pip in /usr/local/lib/python3.7/dist-packages (from fastai) (19.3.1)\n",
            "Requirement already satisfied, skipping upgrade: spacy<4 in /usr/local/lib/python3.7/dist-packages (from fastai) (2.2.4)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fastai) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fastai) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fastai) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fastai) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->fastai) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas->fastai) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->fastai) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<1.9,>=1.7.0->fastai) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->fastai) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai) (1.3.1)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->fastai) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (2.0.5)\n",
            "Requirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (0.8.2)\n",
            "Requirement already satisfied, skipping upgrade: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (7.4.0)\n",
            "Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (1.0.5)\n",
            "Requirement already satisfied, skipping upgrade: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (1.0.5)\n",
            "Requirement already satisfied, skipping upgrade: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (1.1.3)\n",
            "Requirement already satisfied, skipping upgrade: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (0.4.1)\n",
            "Requirement already satisfied, skipping upgrade: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (56.1.0)\n",
            "Requirement already satisfied, skipping upgrade: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (3.0.5)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->fastai) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<4->fastai) (4.0.1)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<4->fastai) (3.4.1)\n",
            "Installing collected packages: fastcore, fastai\n",
            "  Found existing installation: fastai 1.0.61\n",
            "    Uninstalling fastai-1.0.61:\n",
            "      Successfully uninstalled fastai-1.0.61\n",
            "Successfully installed fastai-2.3.1 fastcore-1.3.20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYFuvczGI3eK"
      },
      "source": [
        "import torch\n",
        "import math\n",
        "import glob\n",
        "import cv2\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import datasets, io, transforms\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from fastai.data.external import untar_data, URLs\n",
        "from PIL import Image, ImageCms\n",
        "from sklearn.model_selection import train_test_split\n",
        "from skimage.color import rgb2lab, lab2rgb\n",
        "from skimage.io import imread\n",
        "from torchvision.utils import save_image\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "# import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AR9GliN8I-iC"
      },
      "source": [
        "coco_imgs = untar_data(URLs.COCO_SAMPLE)\n",
        "coco_imgs = str(coco_imgs) + \"/train_sample\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvmigFu-sj4R"
      },
      "source": [
        "# Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eo-mtCB6BH-C"
      },
      "source": [
        "# class ColorDataset(Dataset):\n",
        "#     def __init__(self, root_path=\"\", split=\"train\", transform=None):\n",
        "#         self.split = split\n",
        "#         self.root_path = root_path\n",
        "#         self.transform = transform\n",
        "\n",
        "#         self.srgb_p = ImageCms.createProfile(\"sRGB\")\n",
        "#         self.lab_p = ImageCms.createProfile(\"LAB\")\n",
        "#         self.rgb2lab = ImageCms.buildTransformFromOpenProfiles(self.srgb_p, self.lab_p, \"RGB\", \"LAB\")\n",
        "\n",
        "#         self.paths = os.listdir(self.root_path)\n",
        "\n",
        "#         self.train_split, self.val_split, self.test_split = torch.utils.data.random_split(\n",
        "#             self.paths,\n",
        "#             [\n",
        "#                 math.floor(0.8 * len(self.paths)),\n",
        "#                 math.floor(0.1 * len(self.paths)),\n",
        "#                 len(self.paths) -  math.floor(0.1 * len(self.paths)) -  math.floor(0.8 * len(self.paths))\n",
        "#             ],\n",
        "#             generator=torch.Generator().manual_seed(442)\n",
        "#         )\n",
        "#         self.splits = {\n",
        "#             \"train\": self.train_split,\n",
        "#             \"val\": self.val_split,\n",
        "#             \"test\": self.test_split,\n",
        "#         }\n",
        "\n",
        "\n",
        "#     def load_images(self):\n",
        "#         return self.df.img_path.apply(self.load_image)\n",
        "    \n",
        "    \n",
        "#     def load_image(self, img_path):\n",
        "#         img = imread(\n",
        "#             os.path.join(\n",
        "#                 self.root_path,\n",
        "#                 img_path\n",
        "#             ),\n",
        "#             # io.image.ImageReadMode.RGB\n",
        "#         )\n",
        "\n",
        "#         img = rgb2lab(img)\n",
        "#         return torch.tensor(img)\n",
        "            \n",
        "#     def __len__(self):\n",
        "#         return len(self.splits[self.split])\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         lab_img = self.load_image(self.splits[self.split][idx])\n",
        "#         H, W, C = lab_img.shape\n",
        "#         lab_img = lab_img.view(C, H, W)\n",
        "#         if self.transform:\n",
        "#             lab_img = self.transform(lab_img)\n",
        "#         print(lab_img[0].shape)\n",
        "#         return lab_img[0], lab_img[1:]\n",
        "class ColorDataset(Dataset):\n",
        "    def __init__(self, root_path=coco_imgs, transform=None, mode='test'):\n",
        "        self.train_transforms = transforms.Compose([\n",
        "          transforms.Resize((256, 256)),\n",
        "          transforms.RandomHorizontalFlip()\n",
        "        ])\n",
        "        self.other_transforms = transforms.Resize((256, 256))\n",
        "        self.mode = mode\n",
        "        \n",
        "        self.paths = root_path\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.paths[idx]).convert(\"RGB\")\n",
        "        if (self.mode == 'train'):\n",
        "          img = self.train_transforms(img)\n",
        "        else:\n",
        "          img = self.other_transforms(img)\n",
        "        img = np.array(img)\n",
        "        img_lab = rgb2lab(img).astype(\"float32\") # Converting RGB to L*a*b\n",
        "        img_lab = transforms.ToTensor()(img_lab)\n",
        "        L = img_lab[[0], ...] / 50. - 1. # Between -1 and 1\n",
        "        ab = img_lab[[1, 2], ...] / 110. # Between -1 and 1\n",
        "        \n",
        "        return L, ab\n",
        "    def __len__(self):\n",
        "        return len(self.paths)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdVi-f-5sPiB"
      },
      "source": [
        "# Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vghieWrTsR2W"
      },
      "source": [
        "def encoder(in_channels=0,out_channels=0,first=False):\n",
        "  if first:\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(in_channels,out_channels, kernel_size=4,stride=2,padding=1),\n",
        "        nn.LeakyReLU(0.2)\n",
        "    )\n",
        "  else:\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(in_channels,out_channels, kernel_size=4,stride=2,padding=1),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.LeakyReLU(0.2)\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3cmMdyAsVPv"
      },
      "source": [
        "def decoder(in_channels=0,out_channels=0,dropout = False):\n",
        "  if dropout:\n",
        "    return nn.Sequential(\n",
        "        nn.ConvTranspose2d(in_channels,out_channels, kernel_size=4,stride=2,padding=1),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.Dropout2d(),\n",
        "    )\n",
        "  else:\n",
        "    return nn.Sequential(\n",
        "      nn.ConvTranspose2d(in_channels,out_channels, kernel_size=4,stride=2,padding=1),\n",
        "      nn.BatchNorm2d(out_channels),\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27JB4cj1sWLC"
      },
      "source": [
        "class Generator(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Generator, self).__init__()\n",
        "\n",
        "    self.encoder1 = encoder(in_channels=1,   out_channels=64,first = True)\n",
        "    self.encoder2 = encoder(in_channels=64,  out_channels=128)\n",
        "    self.encoder3 = encoder(in_channels=128, out_channels=256)\n",
        "    self.encoder4 = encoder(in_channels=256, out_channels=512)\n",
        "    self.encoder5 = encoder(in_channels=512, out_channels=512)\n",
        "    self.encoder6 = encoder(in_channels=512, out_channels=512)\n",
        "    self.encoder7 = encoder(in_channels=512, out_channels=512)\n",
        "\n",
        "    self.conv1 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=512,out_channels=512,kernel_size=4,stride=2,padding=1),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "\n",
        "    self.decoder1 = decoder(in_channels=512, out_channels=512, dropout=True)\n",
        "    self.decoder2 = decoder(in_channels=1024, out_channels=512, dropout=True)\n",
        "    self.decoder3 = decoder(in_channels=1024, out_channels=512, dropout=True)\n",
        "    self.decoder4 = decoder(in_channels=1024, out_channels=512)\n",
        "    self.decoder5 = decoder(in_channels=1024, out_channels=256)\n",
        "    self.decoder6 = decoder(in_channels=512, out_channels=128)\n",
        "    self.decoder7 = decoder(in_channels=256, out_channels=64)\n",
        "\n",
        "    self.conv2 = nn.Sequential(\n",
        "        nn.ConvTranspose2d(in_channels = 128, out_channels = 2,kernel_size=4,stride=2,padding=1),\n",
        "        nn.Tanh()\n",
        "    )\n",
        "\n",
        "  def encode(self,x):\n",
        "    skips = []\n",
        "    #print(x.shape)\n",
        "    x = self.encoder1(x)\n",
        "    skips.append(x)\n",
        "    #print(x.shape)\n",
        "    x = self.encoder2(x)\n",
        "    skips.append(x)\n",
        "    #print(x.shape)\n",
        "\n",
        "    x = self.encoder3(x)\n",
        "    skips.append(x)\n",
        "    #print(x.shape)\n",
        "\n",
        "    x = self.encoder4(x)\n",
        "    skips.append(x)\n",
        "    #print(x.shape)\n",
        "\n",
        "    x = self.encoder5(x)\n",
        "    skips.append(x)\n",
        "    #print(x.shape)\n",
        "\n",
        "    x = self.encoder6(x)\n",
        "    skips.append(x)\n",
        "    #print(x.shape)\n",
        "\n",
        "    x = self.encoder7(x)\n",
        "    skips.append(x)\n",
        "    #print(x.shape)\n",
        "\n",
        "    x = self.conv1(x)\n",
        "    #print(x.shape)\n",
        "\n",
        "    return x, skips\n",
        "\n",
        "  def decode(self, x, skips):\n",
        "    #print('decoding',x.shape)\n",
        "    x = self.decoder1(x)\n",
        "    x = F.relu(x)\n",
        "    x = torch.cat([x,skips[6]],1)\n",
        "    #print(x.shape)\n",
        "\n",
        "    x = self.decoder2(x)\n",
        "    x = torch.cat([x,skips[5]],1)\n",
        "    x = F.relu(x)\n",
        "\n",
        "    x = self.decoder3(x)\n",
        "    x = torch.cat([x,skips[4]],1)\n",
        "    x = F.relu(x)\n",
        "\n",
        "    x = self.decoder4(x)\n",
        "    x = torch.cat([x,skips[3]],1)\n",
        "    x = F.relu(x)\n",
        "\n",
        "    x = self.decoder5(x)\n",
        "    x = torch.cat([x,skips[2]],1)\n",
        "    x = F.relu(x)\n",
        "\n",
        "    x = self.decoder6(x)\n",
        "    x = torch.cat([x,skips[1]],1)\n",
        "    x = F.relu(x)\n",
        "\n",
        "    x = self.decoder7(x)\n",
        "    x = torch.cat([x,skips[0]],1)\n",
        "    x = F.relu(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "  def forward(self,x):\n",
        "    x, skips = self.encode(x)\n",
        "    out = self.decode(x,skips)\n",
        "    return self.conv2(out)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvE4QVRd0raN"
      },
      "source": [
        "def test():\n",
        "    x = torch.randn((2,1,256,256))\n",
        "    g = Generator()\n",
        "    print(g.eval())\n",
        "    preds = g(x)\n",
        "    print(preds.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_zMkpps02Cl"
      },
      "source": [
        "test()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdUbqZAnsYbL"
      },
      "source": [
        "# Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcQcfAk2saxZ"
      },
      "source": [
        "class CNNBlock(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=2):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=in_channels,out_channels=out_channels, kernel_size=4, stride=stride, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.LeakyReLU(0.2),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PuGmFiRsdgd"
      },
      "source": [
        "class Discriminator(torch.nn.Module):\n",
        "    def __init__(self, in_channels=3, dims=[64,128,256,512]):\n",
        "        super().__init__()\n",
        "        self.first = nn.Sequential(\n",
        "            nn.Conv2d(3,dims[0], kernel_size=4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2),\n",
        "        )\n",
        "\n",
        "        layers = []\n",
        "        in_channels = dims[0]\n",
        "        for dim in dims[1:]:\n",
        "            layers.append(\n",
        "                CNNBlock(in_channels, dim, stride=1 if dim == dims[-1] else 2), # stride only differs for last cnn block\n",
        "            )\n",
        "            in_channels = dim\n",
        "        \n",
        "        layers.append(\n",
        "            nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=1)\n",
        "        )\n",
        "        \n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        x = torch.cat([x,y], dim=1)\n",
        "        #print(x.shape)\n",
        "        x = self.first(x)\n",
        "        return self.model(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUzr38Ufv2Ws"
      },
      "source": [
        "def test():\n",
        "    x = torch.randn((1,1,256,256))\n",
        "    y = torch.randn((1,2,256,256))\n",
        "    d = Discriminator()\n",
        "    print(d.eval())\n",
        "    preds = d(x,y)\n",
        "    print(preds.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrQNuVQiwKU2"
      },
      "source": [
        "test()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Je_yNKFS1--e"
      },
      "source": [
        "# Training Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7xf4B7kxDzn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "np8GhpfV2A3A"
      },
      "source": [
        "def lab_to_rgb(L, ab):\n",
        "    \"\"\"\n",
        "    Takes a batch of images\n",
        "    \"\"\"\n",
        "    \n",
        "    L = (L + 1.) * 50.\n",
        "    ab = ab * 110.\n",
        "    Lab = torch.cat([L, ab], dim=1).permute(0, 2, 3, 1).cpu().numpy()\n",
        "    rgb_imgs = []\n",
        "    for img in Lab:\n",
        "        img_rgb = lab2rgb(img)\n",
        "        rgb_imgs.append(img_rgb)\n",
        "    return np.stack(rgb_imgs, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxvQ21UEsqu7"
      },
      "source": [
        "# Training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hv4L98dGe6Xo"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lx7aYNaPe8ps"
      },
      "source": [
        "model_save_name = 'classifier.pt'\n",
        "path = F\"/content/gdrive/My Drive/{model_save_name}\" \n",
        "output_path = \"/content/gdrive/My Drive/GAN_output/\" "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNiS5PUq1vA7"
      },
      "source": [
        "d = Discriminator(in_channels=3).to(device)\n",
        "g = Generator().to(device)\n",
        "opt_d = optim.Adam(d.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
        "opt_g = optim.Adam(g.parameters(), lr=2e-4, betas=(0.5, 0.999))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqB24VHz3QDz"
      },
      "source": [
        "BCE = nn.BCEWithLogitsLoss()\n",
        "L1_loss = nn.L1Loss()\n",
        "\n",
        "trans = transforms.Compose([\n",
        "  transforms.Resize((256, 256)),\n",
        "  transforms.RandomHorizontalFlip()\n",
        "])\n",
        "\n",
        "# The batch size we'll use for training\n",
        "batch_size = 16\n",
        "num_epochs = 100\n",
        "L1_LAMBDA = 100 #used in paper\n",
        "val_split = 0.1\n",
        "\n",
        "paths = glob.glob(coco_imgs + \"/*.jpg\")\n",
        "np.random.seed(123) # Seeding for reproducible results\n",
        "paths_subset = np.random.choice(paths, 12_000, replace=False) # Randomly choosing 10,000 images\n",
        "rand_idxs = np.random.permutation(12_000) # Shuffling the indexes\n",
        "train_idxs = rand_idxs[:8000]\n",
        "val_idxs = rand_idxs[8000:10000]\n",
        "test_idxs = rand_idxs[10000:12000]\n",
        "train_paths = paths_subset[train_idxs] \n",
        "val_paths = paths_subset[val_idxs]\n",
        "test_paths = paths_subset[test_idxs]\n",
        "\n",
        "train_data = ColorDataset(train_paths, transform=trans, mode='train')\n",
        "train_dataloader = DataLoader(train_data, batch_size=batch_size, num_workers=4, drop_last=True) #num_workers and shuffle?\n",
        "\n",
        "g_scale = torch.cuda.amp.GradScaler()\n",
        "d_scale = torch.cuda.amp.GradScaler()\n",
        "\n",
        "val_data = ColorDataset(val_paths, transform=trans, mode='val')\n",
        "val_dataloader = DataLoader(val_data, batch_size=8, num_workers=4, drop_last=True) #num_workers and shuffle?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFx_N-jT4tsi"
      },
      "source": [
        "# checkpoint = torch.load(\"./model.pt\")\n",
        "# d.load_state_dict(checkpoint['discriminator_state'])\n",
        "# g.load_state_dict(checkpoint['generator_state'])\n",
        "# opt_d.load_state_dict(checkpoint['discriminator_optim_state'])\n",
        "# opt_g.load_state_dict(checkpoint['generator_optim_state'])\n",
        "# epoch = checkpoint['epoch']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88nCU6rG4YPD"
      },
      "source": [
        "G_losses = []\n",
        "D_losses = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    loop = tqdm(train_dataloader, leave=True)\n",
        "\n",
        "    for idx, (l, ab) in enumerate(loop):\n",
        "        l, ab = l.to(device), ab.to(device)\n",
        "\n",
        "        #Discriminator train\n",
        "        \n",
        "        #l = l.to(torch.float32)\n",
        "        ab_fake = g(l)\n",
        "        D_real = d(l, ab)\n",
        "        D_fake = d(l, ab_fake.detach())\n",
        "\n",
        "        D_real_l = BCE(D_real, torch.ones_like(D_real))\n",
        "        D_fake_l = BCE(D_fake, torch.zeros_like(D_fake))\n",
        "\n",
        "        D_loss = (D_real_l + D_fake_l) / 2 #similar to cyclegan divide by 2, reason being trains discriminator slower relative to generator according to paper, doesnt make much sense?\n",
        "            \n",
        "        #loss.backward(retain_graph=True)\n",
        "        opt_d.zero_grad()\n",
        "        D_loss.backward()\n",
        "        opt_d.step()\n",
        "\n",
        "        #Generator train\n",
        "        \n",
        "        D_fake = d(l, ab_fake)\n",
        "        G_fake_l = BCE(D_fake, torch.ones_like(D_fake))\n",
        "        L1 = L1_loss(ab_fake, ab) * L1_LAMBDA\n",
        "        G_loss = G_fake_l + L1\n",
        "        \n",
        "        opt_g.zero_grad()\n",
        "        G_loss.backward()\n",
        "        opt_g.step()\n",
        "    torch.save({\n",
        "        \"discriminator_state\": d.state_dict(),\n",
        "        \"generator_state\": g.state_dict(),\n",
        "        \"discriminator_optim_state\": opt_d.state_dict(),\n",
        "        \"generator_optim_state\": opt_g.state_dict(),\n",
        "        \"epoch\": epoch,\n",
        "    }, \"./model.pt\")\n",
        "    print(\"Discriminator loss:\", D_loss)\n",
        "    D_losses.append(D_loss)\n",
        "    print(\"Generator loss:\", G_loss)\n",
        "    G_losses.append(G_loss)\n",
        "    l_, ab_ = next(iter(val_dataloader))\n",
        "    l_, ab_ = l_.to(device), ab_.to(device)\n",
        "    g.eval()\n",
        "    with torch.no_grad():\n",
        "        fake_color = g(l_).detach()\n",
        "        real_color = ab_\n",
        "        L = l_\n",
        "        fake_imgs = lab_to_rgb(L, fake_color)\n",
        "        real_imgs = lab_to_rgb(L, real_color)\n",
        "        fig = plt.figure(figsize=(15, 8))\n",
        "        for i in range(5):\n",
        "            ax = plt.subplot(3, 5, i + 1)\n",
        "            ax.imshow(L[i][0].cpu(), cmap='gray')\n",
        "            ax.axis(\"off\")\n",
        "            ax = plt.subplot(3, 5, i + 1 + 5)\n",
        "            ax.imshow(fake_imgs[i])\n",
        "            ax.axis(\"off\")\n",
        "            ax = plt.subplot(3, 5, i + 1 + 10)\n",
        "            ax.imshow(real_imgs[i])\n",
        "            ax.axis(\"off\")\n",
        "        plt.show()\n",
        "    g.train()\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Om5eJT1UdkgD"
      },
      "source": [
        "torch.save({\n",
        "    \"discriminator_state\": d.state_dict(),\n",
        "    \"generator_state\": g.state_dict(),\n",
        "    \"discriminator_optim_state\": opt_d.state_dict(),\n",
        "    \"generator_optim_state\": opt_g.state_dict(),\n",
        "    \"epoch\": epoch,\n",
        "}, path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKUfuCSg2V6y"
      },
      "source": [
        "test_data = ColorDataset(test_paths, transform=trans, mode='test')\n",
        "test_dataloader = DataLoader(test_data, batch_size=8, num_workers=4, drop_last=True) #num_workers and shuffle?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cp6sNVGK64mG"
      },
      "source": [
        "checkpoint = torch.load(path)\n",
        "d.load_state_dict(checkpoint['discriminator_state'])\n",
        "g.load_state_dict(checkpoint['generator_state'])\n",
        "opt_d.load_state_dict(checkpoint['discriminator_optim_state'])\n",
        "opt_g.load_state_dict(checkpoint['generator_optim_state'])\n",
        "epoch = checkpoint['epoch']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSEeFFzEbHlt"
      },
      "source": [
        "psnr_vals = []\n",
        "\n",
        "j = 0\n",
        "\n",
        "for idx, (L, ab) in enumerate(test_dataloader):\n",
        "    L, ab = L.to(device), ab.to(device)\n",
        "    fake_color = g(L).detach()\n",
        "    real_color = ab\n",
        "    #print(fake_color.shape)\n",
        "    if (j % 50):\n",
        "        fake_imgs = lab_to_rgb(L, fake_color)\n",
        "        real_imgs = lab_to_rgb(L, real_color)\n",
        "        for i in range(len(fake_imgs)):\n",
        "            f = plt.figure()\n",
        "            ax1 = f.add_subplot(1,2, 1)\n",
        "            plt.imshow(fake_imgs[i])\n",
        "            ax2= f.add_subplot(1,2, 2)\n",
        "            plt.imshow(real_imgs[i])\n",
        "\n",
        "            ax1.set_title('Fake')\n",
        "            ax2.set_title('Real')\n",
        "\n",
        "            plt.show(block=True)\n",
        "    psnr_vals.append(cv2.PSNR(fake_imgs[0], real_imgs[0]))\n",
        "    j += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kygs1oD0b0Wx"
      },
      "source": [
        "print('Average PSNR: ', sum(psnr_vals) / len(psnr_vals))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPIjMSEGfKVH"
      },
      "source": [
        "plt.figure(figsize=(9, 3))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(G_losses)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Generator Loss\")\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(D_losses)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Discriminator Loss\")\n",
        "plt.show"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdOuYPIdfY5N"
      },
      "source": [
        "\"Random image color testing\"\n",
        "\n",
        "psnr_vals_random = []\n",
        "i = 0\n",
        "\n",
        "for idx, (L, ab) in enumerate(test_dataloader):\n",
        "    L, ab = L.to(device), ab.to(device)\n",
        "    fake_color = torch.randn((8,2,256,256)).to(device)\n",
        "    real_color = ab\n",
        "    #print(fake_color.shape)\n",
        "    if (i % 100 == 0):\n",
        "        fake_imgs = lab_to_rgb(L, fake_color)\n",
        "        real_imgs = lab_to_rgb(L, real_color)\n",
        "        for i in range(len(fake_imgs)):\n",
        "          f = plt.figure()\n",
        "          ax1 = f.add_subplot(1,2, 1)\n",
        "          plt.imshow(fake_imgs[i])\n",
        "          ax2= f.add_subplot(1,2, 2)\n",
        "          plt.imshow(real_imgs[i])\n",
        "\n",
        "          ax1.set_title('Fake')\n",
        "          ax2.set_title('Real')\n",
        "\n",
        "          plt.show(block=True)\n",
        "    psnr_vals_random.append(cv2.PSNR(fake_imgs[0], real_imgs[0]))\n",
        "    i += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-H9ZjQDjP_T"
      },
      "source": [
        "print('Average PSNR Random: ', sum(psnr_vals_random) / len(psnr_vals_random))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}